{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOOV69lETfSbw4vpNjYCM+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "224cfe31ad334e82bf698c31635db689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13041c20ca7148f5aa11bb058eba9889",
              "IPY_MODEL_13fc62ef5511449ea06b1f07d03e3aa2",
              "IPY_MODEL_bbff700aba7547caa38e10d6c5a68b93"
            ],
            "layout": "IPY_MODEL_2ce1f5f3dd66407b8dbae9b0c41ca564"
          }
        },
        "13041c20ca7148f5aa11bb058eba9889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac065499d2a246f280476b447a101b73",
            "placeholder": "​",
            "style": "IPY_MODEL_be1f514c3060459390ee5ac9a17f5b05",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "13fc62ef5511449ea06b1f07d03e3aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34464aa9ff124523aa496a413f0c0885",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7461f2d477b4deba217c37b161fd686",
            "value": 2
          }
        },
        "bbff700aba7547caa38e10d6c5a68b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62988b8b6d6c41aabbef6e1bbf09f159",
            "placeholder": "​",
            "style": "IPY_MODEL_440c483f599d45b79d0f0422965bbf37",
            "value": " 2/2 [00:08&lt;00:00,  3.86s/it]"
          }
        },
        "2ce1f5f3dd66407b8dbae9b0c41ca564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac065499d2a246f280476b447a101b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1f514c3060459390ee5ac9a17f5b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34464aa9ff124523aa496a413f0c0885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7461f2d477b4deba217c37b161fd686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62988b8b6d6c41aabbef6e1bbf09f159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440c483f599d45b79d0f0422965bbf37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlussati/oficina-genai/blob/master/Dia_02_Setup_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Setup"
      ],
      "metadata": {
        "id": "tEhxLtTVT4Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade transformers accelerate sentence-transformers faiss-cpu bitsandbytes\n",
        "\n",
        "import torch, textwrap, os, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "CANDIDATE_MODELS = [\n",
        "#    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "#    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "#    \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    \"microsoft/phi-2\",\n",
        "#    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "]\n",
        "\n",
        "def try_load(model_id, prefer_4bit=True):\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    kwargs = {}\n",
        "    if has_gpu and prefer_4bit:\n",
        "        kwargs = dict(\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            load_in_4bit=True\n",
        "        )\n",
        "    elif has_gpu:\n",
        "        kwargs = dict(\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    else:\n",
        "        kwargs = dict(\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    tok = AutoTokenizer.from_pretrained(model_id)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "    gen_max = 512 if \"Phi-3\" in model_id or \"phi-2\" in model_id else 384\n",
        "    pipe = pipeline(\"text-generation\", model=mdl, tokenizer=tok, max_new_tokens=gen_max)\n",
        "    return pipe, tok, model_id, gen_max\n",
        "\n",
        "pipe = None\n",
        "tok = None\n",
        "model_name = None\n",
        "max_new = 384\n",
        "\n",
        "errors = []\n",
        "for mid in CANDIDATE_MODELS:\n",
        "    try:\n",
        "        pipe, tok, model_name, max_new = try_load(mid)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        errors.append(f\"{mid}: {e}\")\n",
        "\n",
        "if pipe is None:\n",
        "    raise RuntimeError(\"Falha ao carregar todos os modelos. Detalhes:\\n\" + \"\\n\".join(errors))\n",
        "\n",
        "print(\"✅ Modelo carregado:\", model_name)\n",
        "\n",
        "# Formatação ChatML (Phi-3)\n",
        "def format_chat_phi(system, user):\n",
        "    return f\"<|system|>\\n{system}\\n<|end|>\\n<|user|>\\n{user}\\n<|end|>\\n<|assistant|>\\n\"\n",
        "\n",
        "def chat(prompt, system=\"Você é um assistente útil e conciso.\", temperature=0.7):\n",
        "    text = format_chat_phi(system, prompt)\n",
        "    out = pipe(text, do_sample=True, temperature=temperature)[0][\"generated_text\"]\n",
        "    if \"<|assistant|>\" in out:\n",
        "        out = out.split(\"<|assistant|>\")[-1]\n",
        "    out = re.split(r\"<\\|end\\|>|<\\|user\\|>|<\\|system\\|>\", out)[0]\n",
        "    return out.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "224cfe31ad334e82bf698c31635db689",
            "13041c20ca7148f5aa11bb058eba9889",
            "13fc62ef5511449ea06b1f07d03e3aa2",
            "bbff700aba7547caa38e10d6c5a68b93",
            "2ce1f5f3dd66407b8dbae9b0c41ca564",
            "ac065499d2a246f280476b447a101b73",
            "be1f514c3060459390ee5ac9a17f5b05",
            "34464aa9ff124523aa496a413f0c0885",
            "a7461f2d477b4deba217c37b161fd686",
            "62988b8b6d6c41aabbef6e1bbf09f159",
            "440c483f599d45b79d0f0422965bbf37"
          ]
        },
        "id": "iy1iik4RO2zP",
        "outputId": "fb381206-d597-4913-99f8-4066969559d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "224cfe31ad334e82bf698c31635db689"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modelo carregado: microsoft/Phi-3.5-mini-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) LLM — Olá, Mundo"
      ],
      "metadata": {
        "id": "nRWyYNkmT-NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Exemplo 1: IA Generativa para programadores\")\n",
        "resp = chat(\"Explique IA Generativa em 4 frases e dê 1 exemplo prático para programadores (ex.: documentação de funções).\", temperature=0.7)\n",
        "print(textwrap.fill(resp, 100))\n",
        "\n",
        "print(\"\\n### Exemplo 1B: Explicar código em linguagem natural\")\n",
        "codigo = \"\"\"\n",
        "def soma_pares(nums):\n",
        "    return sum(n for n in nums if n % 2 == 0)\n",
        "\"\"\"\n",
        "prompt = f\"Explique em linguagem natural (3-4 frases) o que o código Python abaixo faz e mostre um exemplo de entrada e saída.\\n\\n```python\\n{codigo}\\n```\"\n",
        "print(textwrap.fill(chat(prompt, temperature=0.3), 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTK19Yt4T-0m",
        "outputId": "37a3ff7e-aff5-479c-a7aa-ca0ca5edb8eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Exemplo 1: IA Generativa para programadores\n",
            "A IA Generativa refere-se à criação de novo conteúdo ou solu diffusão de modelos por meio de\n",
            "algoritmos de aprendizado de máquina, particularmente aprendizado profundo. Esses modelos aprendem\n",
            "padrões e podem gerar novas instâncias desses padrões, como imagens ou textos, com destreza sem\n",
            "intervenção humana direta. Por exemplo, na documentação de funções, a IA Generativa pode auto-gerar\n",
            "exemplos de entradas e saídas, assimilando padrões lógicos e formatos, oferecendo cenários práticos\n",
            "para programadores testarem e entenderem a funcionalidade da função. Este processo automatizado\n",
            "melhora a qualidade e a utilidade da documentação, tornando-a mais acessível e relevante para\n",
            "desenvolvedores.  Exemplo Prático: Uma função de uma API gera cartões de crédito com números\n",
            "aleatórios. Usando IA Generativa, podemos criar um modelo que produz exemplos realistas de cartões\n",
            "de crédito, como \"4567 8901 2345 6789 01\" e \"123 45 6789 0123 45678\", para programadores testarem a\n",
            "integridade e o comportamento das chamadas da API com diferentes padrões de dados de cartões de\n",
            "crédito.\n",
            "\n",
            "### Exemplo 1B: Explicar código em linguagem natural\n",
            "O código Python fornecido define uma função chamada `soma_pares` que recebe um iterável `nums` como\n",
            "argumento. A função itera sobre cada número na lista `nums`, verifica se ele é par (ou seja, possui\n",
            "um resto de 0 quando dividido por 2) e, se for, o soma com todos os números pares. Finalmente, a\n",
            "função retorna a soma desses números pares.   Exemplo de entrada: `nums = [2, 3, 4, 6, 8]`  Saída\n",
            "esperada: `20` (porque 2 + 4 + 6 + 8 = 20)   Quando você chama `soma_pares(nums)`, a função\n",
            "processará a lista e retornará a soma dos números pares, que neste caso seria `20`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Prompt — Projetando melhores respostas (algoritmos & código)"
      ],
      "metadata": {
        "id": "tL3jLXy4UC-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Exemplo 2A: Prompt simples (pode ser genérico)\")\n",
        "print(chat(\"Explique o que é complexidade de algoritmo.\"))\n",
        "\n",
        "print(\"\\n### Exemplo 2B: Persona + Formato + Restrições (melhora a clareza)\")\n",
        "prompt = \"\"\"Você é um professor de Algoritmos.\n",
        "Explique o que é complexidade de algoritmo usando o exemplo de busca binária.\n",
        "Mostre a diferença entre O(n) e O(log n) em 5 linhas e termine com uma tabela de 3 linhas comparando casos.\n",
        "\"\"\"\n",
        "print(chat(prompt))\n",
        "\n",
        "print(\"\\n### Exemplo 2C: Debug guiado por prompt\")\n",
        "codigo_bugado = \"\"\"\n",
        "def fatorial(n):\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    return n * fatorial(n-1)\n",
        "\"\"\"\n",
        "prompt_debug = f\"\"\"Você é um revisor de código.\n",
        "Encontre o bug na função Python a seguir e corrija. Explique o motivo do bug em 2 frases e forneça a versão correta.\n",
        "```python\n",
        "{codigo_bugado}\n",
        "```\"\"\"\n",
        "print(chat(prompt_debug, temperature=0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSmZbqS9UDe4",
        "outputId": "3f221f35-d710-47d6-d91a-2bbe1351a7ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Exemplo 2A: Prompt simples (pode ser genérico)\n",
            "Complexidade de algoritmo refere-se à taxa de crescimento da complexidade computacional de um algoritmo, geralmente com base no tamanho da entrada. Essa medição ajuda a determinar a eficiência e escalabilidade do algoritmo em diferentes cenários.\n",
            "\n",
            "Existem várias métricas de complexidade comumente usadas:\n",
            "\n",
            "1. Complexidade Temporal (Temporal)\n",
            "- O tempo constante (O(1)): o tempo para executar o algoritmo não depende do tamanho da entrada dada; é constante, independentemente do tamanho.\n",
            "- Tempo linear (O(n)): o tempo de execução aumenta linearmente com o tamanho da entrada. Por exemplo, percorrer um array ou uma lista que tem 'n' elementos leva O(n) de tempo.\n",
            "- Tempo quadrático (O(n^2)): o tempo de execução aumenta com o quadrado do tamanho da entrada. Isso frequentemente ocorre em algoritmos com laços internos, onde cada iteração pode ser necessária para todos os elementos da entrada.\n",
            "- Tempo exponencial (O(2^n)): a complexidade temporar é uma função exponencial da dimensão da entrada. Exemplos incluem alguns algoritmos de travessia de grafos ou busca binária.\n",
            "\n",
            "2. Complexidade Espacial (Espacial): o espaço de armazenamento necessário pelo algoritmo pode também ser medido em termos de complexidade. Por exemplo:\n",
            "- A complexidade espacial constante (O(1)): espaço de armazenamento constante, independentemente do tamanho da entrada.\n",
            "- A complexidade espacial linear (O(n)): espaço de armazenamento aumenta linearmente com o tamanho da entrada.\n",
            "\n",
            "Essas complexidades ajudam a avaliar quanto tempo ou espaço um algoritmo consumirá à medida que seus dados de entrada crescem. Além disso, elas permitem comparar eficientemente diferentes algoritmos para uma função ou problema espec\n",
            "\n",
            "### Exemplo 2B: Persona + Formato + Restrições (melhora a clareza)\n",
            "Complexidade de algoritmo mede a relação entre a dimensão do problema e o tempo de execuúlni. Na busca binária, o processo de dividir o intervalo pela metade reduz o tamanho do problema exponencialmente, resultando em complexidade O(log n).\n",
            "\n",
            "Comparação em três linhas:\n",
            "\n",
            "| Tipo de Complexidade | Descrição                                       | Exemplo                  |\n",
            "|-----------------------|-------------------------------------------------|--------------------------|\n",
            "| O(n)                  | Linear, diretamente proporcional ao tamanho n | Busca linear em array    |\n",
            "| O(log n)             | Logarítmico, diminui a cada divisão          | Busca binária em array   |\n",
            "\n",
            "A busca binária tem complexidade O(log n) porque cada divisão reduz o espaço de busca em uma potência de 2, levando a um crescimento logarítmico. Em contraste, a busca linear tem complexidade O(n), movendo-se linearmente pelo array inteiro.\n",
            "\n",
            "| Fator de Rendimento | Busca Binária | Busca Linear |\n",
            "|-------------------------|---------------|---------------|\n",
            "| Eficácia (para grandes N) | Alta          | Baixa          |\n",
            "| Tempo de Execução (para grandes N) | Rápido       | Lento          |\n",
            "| Vantagens               | Melhor na busca por elementos fixos ou com intervalos pré-ordenados. | Simples e direto |\n",
            "\n",
            "A tabela demonstra que a busca binária é mais eficiente em termos de tempo de execução, especialmente para grandes conjuntos de dados, enquanto a busca linear é mais simples mas menos eficiente.\n",
            "\n",
            "### Exemplo 2C: Debug guiado por prompt\n",
            "O bug na função Python fornecida é que ela retorna 0 para o caso base quando `n` é igual a 0, o que não é correto para o cálculo do fatorial. O fatorial de 0 deve ser definido como 1, pois o fatorial de qualquer número maior que 0 é o produto de todos os inteiros positivos até esse número, e quando `n` é 0, não há inteiros positivos a multiplicar.\n",
            "\n",
            "Para corrigir o bug, devemos mudar o caso base para retornar 1 quando `n` é igual a  dia. Aqui está a função corrigida:\n",
            "\n",
            "```python\n",
            "def fatorial(n):\n",
            "    if n == 0:\n",
            "        return 1\n",
            "    return n * fatorial(n - 1)\n",
            "```\n",
            "\n",
            "Agora, com a correção, se você chamar `fatorial(0)`, ele retornará `1`, que é o valor correto para o fatorial de 0. Esta função agora calculará corretamente o fatorial de qualquer inteiro não negativo passado a ela.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Alucinação — detectando e corrigindo respostas erradas (CS)"
      ],
      "metadata": {
        "id": "-21P1955V9o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Exemplo 3A: Pergunta factual sem contexto (pode alucinar)\")\n",
        "pergunta = \"Quem venceu a maratona de Boston em 2015 na categoria masculina?\"\n",
        "print(\"Pergunta:\", pergunta)\n",
        "print(\"Resposta sem contexto:\\n\", chat(pergunta, temperature=0.2))\n",
        "\n",
        "print(\"\\n### Exemplo 3B: Injetando contexto factual (anti-alucinação)\")\n",
        "contexto = \"\"\"Fatos verificados:\n",
        "- Vencedor da Maratona de Boston 2015 (masculino): Lelisa Desisa (Etiópia).\n",
        "- Tempo oficial: 2:09:17.\n",
        "Responda apenas com base nos fatos acima.\n",
        "\"\"\"\n",
        "prompt_ctx = f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\\nResponda de forma direta.\"\n",
        "print(\"Resposta com contexto:\\n\", chat(prompt_ctx, temperature=0.2))\n",
        "\n",
        "print(\"\\n### Exemplo 3C: Exigir 'Não sei' quando não houver evidência\")\n",
        "prompt_nsei = \"\"\"Se você não tiver certeza com base no contexto abaixo, diga \"Não sei\".\n",
        "Contexto:\n",
        "- (vazio)\n",
        "Pergunta: Quem venceu a maratona de Boston em 2015 na categoria masculina?\n",
        "\"\"\"\n",
        "print(\"Resposta com regra de 'Não sei':\\n\", chat(prompt_nsei, temperature=0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr6-gWfJV-OG",
        "outputId": "e706a438-9714-4e41-8270-fdb0a8e37fea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Exemplo 3A: Pergunta factual sem contexto (pode alucinar)\n",
            "Pergunta: Quem venceu a maratona de Boston em 2015 na categoria masculina?\n",
            "Resposta sem contexto:\n",
            " Em 2015, a maratona de Boston na categoria masculina foi vencida por Geoffrey Ronke. Ele completou a prova em um tempo de 2 horas, 37 minutos e 12 segundos, estabelecendo um novo recorde de categoria para Boston. É importante notar que esses dados podem ser verificados através de fontes confiáveis como a página oficial da Boston Marathon ou registros de corridores.\n",
            "\n",
            "Para mais detalhes ou para atualizações mais recentes, recomendo consultar essas fontes diretamente.\n",
            "\n",
            "### Exemplo 3B: Injetando contexto factual (anti-alucinação)\n",
            "Resposta com contexto:\n",
            " Lelisa Desisa\n",
            "\n",
            "### Exemplo 3C: Exigir 'Não sei' quando não houver evidência\n",
            "Resposta com regra de 'Não sei':\n",
            " Não sei\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Exemplo 3A: Pergunta técnica que pode induzir alucinação\")\n",
        "pergunta = \"Qual é a função em Python que calcula a mediana automaticamente no módulo math?\"\n",
        "print(\"Pergunta:\", pergunta)\n",
        "print(\"Resposta sem contexto:\\n\", chat(pergunta, temperature=0.2))\n",
        "\n",
        "print(\"\\n### Exemplo 3B: Injetando contexto correto (anti-alucinação)\")\n",
        "contexto = \"\"\"Fatos verificados:\n",
        "- Em Python, a função de mediana está em statistics: statistics.median().\n",
        "- O módulo math NÃO possui função median().\n",
        "Responda apenas com base nos fatos acima.\n",
        "\"\"\"\n",
        "prompt_ctx = f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\\nResponda de forma direta.\"\n",
        "print(\"Resposta com contexto:\\n\", chat(prompt_ctx, temperature=0.2))\n",
        "\n",
        "print(\"\\n### Exemplo 3C: Regra de 'Não sei' quando não houver evidência\")\n",
        "prompt_nsei = \"\"\"Se você não tiver certeza com base no contexto abaixo, diga \\\"Não sei\\\".\n",
        "Contexto:\n",
        "- (vazio)\n",
        "Pergunta: Qual é a função em Python que calcula a mediana automaticamente no módulo math?\n",
        "\"\"\"\n",
        "print(\"Resposta com 'Não sei':\\n\", chat(prompt_nsei, temperature=0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTqMkKR3aFiW",
        "outputId": "92dfedf5-48e2-4a34-e7b7-470bee1ae6a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Exemplo 3A: Pergunta técnica que pode induzir alucinação\n",
            "Pergunta: Qual é a função em Python que calcula a mediana automaticamente no módulo math?\n",
            "Resposta sem contexto:\n",
            " Na biblioteca padrão de Python, não há uma função `math` que calcule diretamente a mediana. No entandas, você pode calcular a mediana usando a biblioteca `statistics`, que não é parte da biblioteca `math`. Veja como você pode fazer isso:\n",
            "\n",
            "```python\n",
            "import statistics\n",
            "\n",
            "lista = [1, 2, 3, 4, 5]\n",
            "\n",
            "# Calcule a mediana\n",
            "mediana = statistics.median(lista)\n",
            "\n",
            "print(mediana)\n",
            "```\n",
            "\n",
            "Neste exemplo, a função `statistics.median()` calcula a mediana da lista de números. Se sua lista contiver um número par de elementos, a função retornará o número que está no meio, ou se houver empate, ela calculará a média dos dois números centrais.\n",
            "\n",
            "Se você quiser calcular a mediana sem usar a biblioteca `statistics`, você pode fazer isso com as seguintes etapas:\n",
            "\n",
            "```python\n",
            "lista = sorted(lista)\n",
            "n = len(lista)\n",
            "\n",
            "# Para um número par de elementos\n",
            "if n % 2 == 0:\n",
            "    mediana = (lista[n//2 - 1] + lista[n//2]) / 2\n",
            "# Para um número ímpar de elementos\n",
            "else:\n",
            "    mediana = lista[n//2]\n",
            "\n",
            "print(mediana)\n",
            "```\n",
            "\n",
            "Neste código, a lista é primeiro ordenada. Se a lista tiver um número par de elementos, a mediana é calculada como a média dos dois números centrais. Se a lista tiver um número ímpar de elementos, a mediana é o número central.\n",
            "\n",
            "Observe que esses exemplos assumem que você já tem uma lista de números. Se você quiser gerar uma lista aleatória e encontrar a mediana, você pode fazer algo assim:\n",
            "\n",
            "```python\n",
            "import random\n",
            "\n",
            "# Gerar uma lista de 10 números aleatórios\n",
            "numeros = random.sample(range(1, 100), 10)\n",
            "\n",
            "# Calcular a mediana\n",
            "mediana = statistics.median(numeros)\n",
            "\n",
            "print(\"Númer\n",
            "\n",
            "### Exemplo 3B: Injetando contexto correto (anti-alucinação)\n",
            "Resposta com contexto:\n",
            " Não, o módulo math em Python não possui função para calcular a mediana. A função para calcular a mediana está no módulo statistics, especificamente na função statistics.median().\n",
            "\n",
            "### Exemplo 3C: Regra de 'Não sei' quando não houver evidência\n",
            "Resposta com 'Não sei':\n",
            " Não sei.\n",
            "\n",
            "Explicação: O módulo `math` em Python não inclui uma função para calcular a mediana. A função `median()` geralmente é encontrada no módulo `statistics` em Python, não no módulo `math`. Se você não tem certeza, é melhor consultar a documentação ou usar `help()` para obter mais informações.\n",
            "\n",
            "Em Python, você pode calcular a mediana usando o módulo `statistics` da seguinte forma:\n",
            "\n",
            "```python\n",
            "import statistics\n",
            "\n",
            "lista = [1, 2, 5, 6, 3]\n",
            "mediana = statistics.median(lista)\n",
            "print(mediana)  # Output: 3\n",
            "```\n",
            "\n",
            "Lembre-se de que se sua lista de números tiver um número ímpar de elementos, a mediana será o número do meio. Se sua lista tiver um número par de elementos, será a média dos dois números do meio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Mini‑RAG — FAISS + Sentence‑Transformers (local)"
      ],
      "metadata": {
        "id": "GObQEs18WOzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4.1 Base de conhecimento simulada (CS)\n",
        "docs = [\n",
        "(\"python_loops\", \"\"\"Em Python, loops podem ser feitos com for ou while.\n",
        "A função range() é usada para gerar sequências numéricas (ex.: range(0, 10)).\"\"\"),\n",
        "(\"git_basics\", \"\"\"Comandos básicos de Git: git init, git status, git add, git commit -m \"msg\", git push, git pull.\n",
        "Use git branch e git checkout -b para criar e trocar de branch.\"\"\"),\n",
        "(\"linux_cli\", \"\"\"Comandos Linux úteis: ls, cd, mkdir, rm, cp, mv, cat, grep, find.\n",
        "Permissões: chmod, chown. Pacotes: apt, yum (dependendo da distro).\"\"\"),\n",
        "(\"algorithms\", \"\"\"Busca binária tem complexidade O(log n). Ordenação por seleção é O(n^2).\n",
        "Estruturas de dados: listas, pilhas, filas, árvores, grafos.\"\"\"),\n",
        "(\"ml_basics\", \"\"\"Machine Learning aprende padrões a partir de dados. Overfitting ocorre quando o modelo memoriza o treino.\n",
        "Regularização ajuda a reduzir overfitting.\"\"\")\n",
        "]\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "corpus_texts = [d[1] for d in docs]\n",
        "corpus_emb = embedder.encode(corpus_texts, normalize_embeddings=True)\n",
        "index = faiss.IndexFlatIP(corpus_emb.shape[1])\n",
        "index.add(corpus_emb)\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    q_emb = embedder.encode([query], normalize_embeddings=True)\n",
        "    D, I = index.search(np.array(q_emb), k)\n",
        "    ctx = \"\\n\\n\".join([corpus_texts[i] for i in I[0]])\n",
        "    return ctx\n",
        "\n",
        "# Perguntas típicas de computação\n",
        "user_q = \"Como faço um loop em Python e uso range()? Dê um exemplo.\"\n",
        "ctx = retrieve(user_q, k=2)\n",
        "\n",
        "prompt_rag = f\"\"\"Use SOMENTE o contexto a seguir para responder. Se não houver informação suficiente, diga \"Não sei\".\n",
        "Contexto:\n",
        "{ctx}\n",
        "\n",
        "Pergunta: {user_q}\n",
        "Mostre um exemplo de código curto.\n",
        "\"\"\"\n",
        "print(\"Contexto recuperado (trecho):\\n\", ctx[:300], \"...\\n\")\n",
        "print(\"Resposta RAG:\\n\", chat(prompt_rag, temperature=0.2))\n",
        "\n",
        "# Atividade extra: tente com perguntas sobre Git/Linux\n",
        "user_q2 = \"Como criar uma nova branch e trocar para ela no Git?\"\n",
        "ctx2 = retrieve(user_q2, k=2)\n",
        "prompt_rag2 = f\"\"\"Use APENAS o contexto a seguir.\n",
        "Contexto:\n",
        "{ctx2}\n",
        "\n",
        "Pergunta: {user_q2}\n",
        "Responda com comandos diretos.\n",
        "\"\"\"\n",
        "print(\"\\n---\\nPergunta extra (Git):\", user_q2)\n",
        "print(\"Resposta RAG:\\n\", chat(prompt_rag2, temperature=0.2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ER9-yRaWPd2",
        "outputId": "0801b5c8-d476-436e-ee8d-ac0f5c5763e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contexto recuperado (trecho):\n",
            " Em Python, loops podem ser feitos com for ou while.\n",
            "A função range() é usada para gerar sequências numéricas (ex.: range(0, 10)).\n",
            "\n",
            "Busca binária tem complexidade O(log n). Ordenação por seleção é O(n^2).\n",
            "Estruturas de dados: listas, pilhas, filas, árvores, grafos. ...\n",
            "\n",
            "Resposta RAG:\n",
            " Aqui está um exemplo simples de loop em Python usando `range()`:\n",
            "\n",
            "```python\n",
            "for i in range(0, 5):  # Gerará números de 0 a 4\n",
            "    print(i)\n",
            "```\n",
            "\n",
            "Este código irá imprimir os números de 0 a 4 em linhas separadas. A função `range(0, 5)` gera uma sequência de números de 0 a 4 (excluindo 5), e o loop `for` itera sobre cada número e imprime-o.\n",
            "\n",
            "---\n",
            "Pergunta extra (Git): Como criar uma nova branch e trocar para ela no Git?\n",
            "Resposta RAG:\n",
            " Para criar uma nova branch e trocar para ela no Git, use os seguintмерes comandos:\n",
            "\n",
            "```bash\n",
            "git branch novabranch\n",
            "git checkout novabranch\n",
            "```\n",
            "\n",
            "Ou, para criar e mudar para uma nova branch em uma única linha:\n",
            "\n",
            "```bash\n",
            "git checkout -b novabranch\n",
            "```\n",
            "\n",
            "Esses comandos primeiro criam uma nova branch chamada `novabranch` e depois moverão o alvo do trabalho atual para essa nova branch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Agents — agente de suporte técnico (calculadora, KB e docs)"
      ],
      "metadata": {
        "id": "GKq2K1ucavPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tool_calc(expr: str):\n",
        "    try:\n",
        "        return str(eval(expr, {\"__builtins__\": {}}))\n",
        "    except Exception as e:\n",
        "        return f\"Erro na expressão: {e}\"\n",
        "\n",
        "KB = {\n",
        "    \"horarios_biblioteca\": \"A biblioteca abre das 8h às 20h, de segunda a sexta.\",\n",
        "    \"wifi_campus\": \"A rede Wi-Fi do campus é 'UNIV-ALUNOS'; login com RA e senha do portal.\",\n",
        "    \"docs_python_list\": \"https://docs.python.org/3/tutorial/datastructures.html#more-on-lists\",\n",
        "}\n",
        "\n",
        "def tool_kb(key: str):\n",
        "    return KB.get(key.lower(), \"Não encontrei essa informação na base local.\")\n",
        "\n",
        "def tool_doc(topic: str):\n",
        "    base = \"https://docs.python.org/3/search.html?q=\"\n",
        "    return base + topic.replace(\" \", \"+\")\n",
        "\n",
        "TOOLS = {\n",
        "    \"calculator\": tool_calc,\n",
        "    \"kb_lookup\": tool_kb,\n",
        "    \"doc_search\": tool_doc,\n",
        "}\n",
        "\n",
        "AGENT_SYS = \"\"\"Você é um agente que pode chamar ferramentas antes de responder.\n",
        "Se precisar de cálculo use: CALL_TOOL:calculator|<expressao>\n",
        "Se precisar de um fato da base local use: CALL_TOOL:kb_lookup|<chave>\n",
        "Se precisar sugerir documentação oficial use: CALL_TOOL:doc_search|<termo>\n",
        "Se não precisar de ferramenta, responda normalmente.\n",
        "Depois de receber o RESULT_TOOL, produza a resposta final curta e correta.\n",
        "Respeite o formato à risca.\n",
        "\"\"\"\n",
        "\n",
        "def agent(question):\n",
        "    first = chat(\n",
        "        f\"Pergunta do usuário: {question}\\nDecida se precisa de ferramenta. Se sim, use o formato CALL_TOOL:...\",\n",
        "        system=AGENT_SYS,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    if first.strip().startswith(\"CALL_TOOL:\"):\n",
        "        try:\n",
        "            _, rest = first.split(\":\", 1)\n",
        "            tool, arg = rest.split(\"|\", 1)\n",
        "            tool = tool.strip(); arg = arg.strip()\n",
        "            if tool in TOOLS:\n",
        "                result = TOOLS[tool](arg)\n",
        "            else:\n",
        "                result = f\"Ferramenta desconhecida: {tool}\"\n",
        "        except Exception as e:\n",
        "            result = f\"Erro ao interpretar chamada de ferramenta: {e}\"\n",
        "\n",
        "        final = chat(\n",
        "            f\"Pergunta: {question}\\nRESULT_TOOL ({tool}): {result}\\nAgora responda de forma final e direta.\",\n",
        "            system=AGENT_SYS,\n",
        "            temperature=0.2\n",
        "        )\n",
        "        return f\"[{first}]\\n\\n→ Resultado da ferramenta: {result}\\n\\nResposta final: {final}\"\n",
        "    else:\n",
        "        return f\"(Sem ferramenta)\\nResposta: {first}\"\n",
        "\n",
        "print(\"### Exemplo 5A: Cálculo (tempo em segundos)\")\n",
        "print(agent(\"Quantos segundos há em 2 horas?\"))\n",
        "\n",
        "print(\"\\n### Exemplo 5B: Consulta à base local (Wi-Fi do campus)\")\n",
        "print(agent(\"Qual é a rede Wi-Fi do campus?\"))\n",
        "\n",
        "print(\"\\n### Exemplo 5C: Link para documentação oficial\")\n",
        "print(agent(\"Onde aprendo sobre listas em Python?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzbqk3Mwavrt",
        "outputId": "2b208b08-1589-489d-aa7d-68395f092a2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Exemplo 5A: Cálculo (tempo em segundos)\n",
            "[CALL_TOOL:calculator|60*60*2\n",
            "\n",
            "Resposta: 7200 segundos]\n",
            "\n",
            "→ Resultado da ferramenta: Erro na expressão: invalid syntax (<string>, line 3)\n",
            "\n",
            "Resposta final: Há 7200 segundos em 2 horas. (Calculado: 2 horas * 60 minutos/hora * 60 segundos/minuto = 7200 segundos)\n",
            "\n",
            "### Exemplo 5B: Consulta à base local (Wi-Fi do campus)\n",
            "[CALL_TOOL:kb_lookup|campus Wi-Fi\n",
            "\n",
            "---\n",
            "\n",
            "Como assistente de IA, não execute chamadas de sistema no mundo real, mas aqui está como você poderia lidar com a situação:\n",
            "\n",
            "1. O sistema de busca da base de conhecimento (kb_lookup) procuraria informações sobre a rede Wi-Fi no campus.\n",
            "2. Ele retornaria o nome da rede Wi-Fi, possivelmente juntamente com outras informações relevantes, como senha ou detalhes de conexão.\n",
            "3. A informação recuperada seria então fornecida ao usuário.\n",
            "\n",
            "Por exemplo, a resposta poderia ser: \"A rede Wi-Fi do campus é 'UniNet', e sua senha é 'ExemploSenha123'. Certifique-se de usar a mesma senha que você usou para configurar seu dispositivo.\"\n",
            "\n",
            "Lembre-se, na realidade, você precisaria de acesso a uma base de conhecimentos ou banco de dados para realizar esse cálculo ou busca.]\n",
            "\n",
            "→ Resultado da ferramenta: Não encontrei essa informação na base local.\n",
            "\n",
            "Resposta final: Não há informação disponível na base local. Pode ser útil entrar em contato diretamente com o departamento de TI do campus para obter as informações mais precisas.\n",
            "\n",
            "### Exemplo 5C: Link para documentação oficial\n",
            "[CALL_TOOL:doc_search|Python list comprehension\n",
            "\n",
            "---\n",
            "\n",
            "A resposta final curta e correta é: Você pode aprender sobre listas em Python consultando a documentação oficial usando o comando de busca de documentação: `doc_search:Python list comprehension`.]\n",
            "\n",
            "→ Resultado da ferramenta: https://docs.python.org/3/search.html?q=Python+list+comprehension\n",
            "\n",
            "---\n",
            "\n",
            "A+resposta+final+curta+e+correta+é:+Você+pode+aprender+sobre+listas+em+Python+consultando+a+documentação+oficial+usando+o+comando+de+busca+de+documentação:+`doc_search:Python+list+comprehension`.\n",
            "\n",
            "Resposta final: doc_search:Python list comprehension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6wThCJ6Kaz9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}